{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNET_TF_Son.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH7MRsV_QOUe"
      },
      "source": [
        "#Drive Bağlantısını Kurma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhOmFBzrrxa2",
        "outputId": "0b1c8599-4e9b-46d3-9b44-51450e4659dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC2_5w8OQ9wp"
      },
      "source": [
        "#Kütüphaneleri Kurma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NotCxNEcS8_V"
      },
      "source": [
        "from keras.models import Sequential\n",
        "#from scipy.misc import imread\n",
        "get_ipython().magic('matplotlib inline')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import numpy as np\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
        "\n",
        "from sklearn.metrics import log_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3YtSFwZPL8S"
      },
      "source": [
        "# AUGMENTED READ\n",
        "train_path=\"/content/gdrive/My Drive/plant_data/train_last/\"\n",
        "train=pd.read_csv(\"/content/gdrive/My Drive/plant_data/train_last_df.csv\")\n",
        "\n",
        "X_train=[]\n",
        "for i in range(len(train)):\n",
        "\n",
        "    temp_img=image.load_img(train_path+train['image_id'][i]+\".jpg\",target_size=(224,224))\n",
        "\n",
        "    temp_img=image.img_to_array(temp_img)\n",
        "\n",
        "    X_train.append(temp_img)\n",
        "\n",
        "X_train=np.array(X_train)\n",
        "X_train=preprocess_input(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW9Z_qglbfBF",
        "outputId": "d6c2a20d-df47-414c-ba1d-cb4517ceab02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1913"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOMQBOWkbU1i"
      },
      "source": [
        "valid_path=\"/content/gdrive/My Drive/plant_data/valid_last/\"\n",
        "valid=pd.read_csv(\"/content/gdrive/My Drive/plant_data/valid_last_df.csv\")\n",
        "\n",
        "X_valid=[]\n",
        "for i in range(len(valid)):\n",
        "\n",
        "    temp_img=image.load_img(valid_path+valid['image_id'][i]+\".jpg\",target_size=(224,224))\n",
        "\n",
        "    temp_img=image.img_to_array(temp_img)\n",
        "\n",
        "    X_valid.append(temp_img)\n",
        "\n",
        "X_valid=np.array(X_valid)\n",
        "X_valid=preprocess_input(X_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdyc_IstfDyX",
        "outputId": "d2ad4be2-cf71-4c43-f10e-2d53359d7a47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(X_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCSNfTnidlnQ"
      },
      "source": [
        "aug=pd.read_csv(\"/content/gdrive/My Drive/plant_data/train_aug_df.csv\")\n",
        "aug_path=\"/content/gdrive/My Drive/plant_data/train_aug/\"\n",
        "\n",
        "X_aug=[]\n",
        "for i in range(len(aug)):\n",
        "\n",
        "    temp_img=image.load_img(aug_path+aug['image_id'][i]+\".jpg\",target_size=(224,224))\n",
        "\n",
        "    temp_img=image.img_to_array(temp_img)\n",
        "\n",
        "    X_aug.append(temp_img)\n",
        "\n",
        "X_aug=np.array(X_aug)\n",
        "X_aug=preprocess_input(X_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8BtfwXteb6S"
      },
      "source": [
        "X_aug=np.concatenate((X_train, X_aug))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA5BhsGdnjFq",
        "outputId": "756fccf1-ec4c-4825-8917-38cd1d042a69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#check counts of classes\n",
        "aug.sum(axis = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "image_id             Train_1161_0_2Train_314_1Train_1781_0Train_101...\n",
              "healthy                                                           3826\n",
              "multiple_diseases                                                 1913\n",
              "rust                                                                 0\n",
              "scab                                                                 0\n",
              "type                 healthyhealthymultiple_diseaseshealthymultiple...\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIgiB4kpcvJT"
      },
      "source": [
        "train=pd.read_csv(\"/content/gdrive/My Drive/plant_data/train_last_df.csv\")\n",
        "\n",
        "train[\"type\"]=[None]*len(train)\n",
        "for i in range(0,len(train)):\n",
        "  for j in range(1,5):\n",
        "    if train.iloc[i,j]==1:\n",
        "      train.loc[i,\"type\"]=train.columns[j]\n",
        "      Y_train=np.asarray(train['type'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "Y_train = le.fit_transform(Y_train)\n",
        "\n",
        "Y_train=to_categorical(Y_train)\n",
        "\n",
        "Y_train=np.array(Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe8znK40ZT14"
      },
      "source": [
        "valid=pd.read_csv(\"/content/gdrive/My Drive/plant_data/valid_last_df.csv\")\n",
        "\n",
        "valid[\"type\"]=[None]*len(valid)\n",
        "for i in range(0,len(valid)):\n",
        "  for j in range(1,5):\n",
        "    if valid.iloc[i,j]==1:\n",
        "      valid.loc[i,\"type\"]=valid.columns[j]\n",
        "      Y_valid=np.asarray(valid['type'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "Y_valid = le.fit_transform(Y_valid)\n",
        "\n",
        "Y_valid=to_categorical(Y_valid)\n",
        "\n",
        "Y_valid=np.array(Y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GNvZcapfAUT"
      },
      "source": [
        "aug=pd.read_csv(\"/content/gdrive/My Drive/plant_data/train_aug_df.csv\")\n",
        "\n",
        "aug[\"type\"]=[None]*len(aug)\n",
        "for i in range(0,len(aug)):\n",
        "  for j in range(1,5):\n",
        "    if aug.iloc[i,j]==1:\n",
        "      aug.loc[i,\"type\"]=aug.columns[j]\n",
        "      Y_aug=np.asarray(aug['type'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "Y_aug = le.fit_transform(Y_aug)\n",
        "\n",
        "Y_aug=to_categorical(Y_aug)\n",
        "\n",
        "Y_aug=np.array(Y_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rtvmFyOpTto",
        "outputId": "88b55878-3d05-44bb-8448-6a5bd808a827",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable             Type            Data/Info\n",
            "----------------------------------------------\n",
            "Activation           type            <class 'tensorflow.python<...>.layers.core.Activation'>\n",
            "AveragePooling2D     type            <class 'tensorflow.python<...>ooling.AveragePooling2D'>\n",
            "Convolution2D        type            <class 'tensorflow.python<...>rs.convolutional.Conv2D'>\n",
            "Dense                type            <class 'tensorflow.python<...>keras.layers.core.Dense'>\n",
            "Dropout              type            <class 'tensorflow.python<...>ras.layers.core.Dropout'>\n",
            "Flatten              type            <class 'tensorflow.python<...>ras.layers.core.Flatten'>\n",
            "Input                function        <function Input at 0x7f11fe0ad400>\n",
            "LabelEncoder         type            <class 'sklearn.preproces<...>ing._label.LabelEncoder'>\n",
            "MaxPooling2D         type            <class 'tensorflow.python<...>rs.pooling.MaxPooling2D'>\n",
            "Reshape              type            <class 'tensorflow.python<...>ras.layers.core.Reshape'>\n",
            "SGD                  ABCMeta         <class 'tensorflow.python<...>v2.gradient_descent.SGD'>\n",
            "Sequential           type            <class 'tensorflow.python<...>e.sequential.Sequential'>\n",
            "VGG16                function        <function VGG16 at 0x7f11f981d510>\n",
            "X_aug                ndarray         5739x224x224x3: 863880192 elems, type `float32`, 3455520768 bytes (3295.44140625 Mb)\n",
            "X_final              ndarray         7652x224x224x3: 1151840256 elems, type `float32`, 4607361024 bytes (4393.921875 Mb)\n",
            "X_train              ndarray         1913x224x224x3: 287960064 elems, type `float32`, 1151840256 bytes (1098.48046875 Mb)\n",
            "X_valid              ndarray         363x224x224x3: 54641664 elems, type `float32`, 218566656 bytes (208.44140625 Mb)\n",
            "Y_aug                ndarray         5739x2: 11478 elems, type `float32`, 45912 bytes\n",
            "Y_train              ndarray         1913x4: 7652 elems, type `float32`, 30608 bytes\n",
            "Y_valid              ndarray         363x4: 1452 elems, type `float32`, 5808 bytes\n",
            "ZeroPadding2D        type            <class 'tensorflow.python<...>olutional.ZeroPadding2D'>\n",
            "aug                  DataFrame                   image_id  hea<...>\\n[5739 rows x 6 columns]\n",
            "aug_path             str             /content/gdrive/My Drive/plant_data/train_aug/\n",
            "decode_predictions   function        <function decode_predictions at 0x7f11f981d620>\n",
            "drive                module          <module 'google.colab.dri<...>s/google/colab/drive.py'>\n",
            "i                    int             5738\n",
            "image                module          <module 'tensorflow.keras<...>ssing/image/__init__.py'>\n",
            "j                    int             4\n",
            "keras                module          <module 'keras' from '/us<...>kages/keras/__init__.py'>\n",
            "le                   LabelEncoder    LabelEncoder()\n",
            "log_loss             function        <function log_loss at 0x7f11f0065158>\n",
            "merge                module          <module 'keras.layers.mer<...>s/keras/layers/merge.py'>\n",
            "np                   module          <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n",
            "pd                   module          <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n",
            "plt                  module          <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
            "preprocess_input     function        <function preprocess_input at 0x7f11f981d598>\n",
            "temp_img             ndarray         224x224x3: 150528 elems, type `float32`, 602112 bytes (588.0 kb)\n",
            "to_categorical       function        <function to_categorical at 0x7f11f9043f28>\n",
            "train                DataFrame                image_id  health<...>\\n[1913 rows x 6 columns]\n",
            "train_path           str             /content/gdrive/My Drive/plant_data/train_last/\n",
            "valid                DataFrame              image_id  healthy <...>n\\n[363 rows x 6 columns]\n",
            "valid_path           str             /content/gdrive/My Drive/plant_data/valid_last/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP4Rv3gsnpXF",
        "outputId": "688a8bc5-67da-4f84-d3b5-3fd7f73531e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y_aug=np.concatenate((Y_train, Y_aug))\n",
        "len(Y_aug)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7652"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSHWbnWCE9-M"
      },
      "source": [
        "# not used because we use pre-splitted data\n",
        "train=pd.read_csv(\"/content/gdrive/My Drive/plant_data/train.csv\")\n",
        "train_path=\"/content/gdrive/My Drive/plant_data/images/\"\n",
        "\n",
        "#read train data set to be splitted to train and valid\n",
        "train_img=[]\n",
        "for i in range(len(train)):\n",
        "\n",
        "    temp_img=image.load_img(train_path+train['image_id'][i]+\".jpg\",target_size=(224,224))\n",
        "\n",
        "    temp_img=image.img_to_array(temp_img)\n",
        "\n",
        "    train_img.append(temp_img)\n",
        "\n",
        "train_img=np.array(train_img)\n",
        "train_img=preprocess_input(train_img)\n",
        "\n",
        "\n",
        "train[\"type\"]=[None]*len(train)\n",
        "for i in range(0,len(train)):\n",
        "  for j in range(1,5):\n",
        "    if train.iloc[i,j]==1:\n",
        "      train.loc[i,\"type\"]=train.columns[j]\n",
        "\n",
        "\n",
        "train_y=np.asarray(train['type'])\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "train_y = le.fit_transform(train_y)\n",
        "\n",
        "train_y=to_categorical(train_y)\n",
        "\n",
        "train_y=np.array(train_y)\n",
        "\n",
        "#train valid split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aUhvlIGqN2C"
      },
      "source": [
        "#RESNET 256 pooling batchnorm, double droput\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "def ResNet(img_rows, img_cols, channel=1, num_classes=None):\n",
        "\n",
        "    model = ResNet50(weights='imagenet', include_top=True)\n",
        "\n",
        "    #model.layers.pop()\n",
        "\n",
        "    #model.outputs = [model.layers[-1].output]\n",
        "\n",
        "    #model.layers[-1].outbound_nodes1 = []\n",
        "\n",
        "    x = tf.keras.layers.Dropout(.2)(model.layers[-35].output)\n",
        "\n",
        "    x=tf.keras.layers.Conv2D(256,1,kernel_regularizer=regularizers.l2(0.8))(x)\n",
        "\n",
        "    x=tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x=Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.8))(x)\n",
        "\n",
        "    x = tf.keras.layers.Dropout(.2)(x)\n",
        "\n",
        "    x=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x=Dense(num_classes, activation='softmax')(x)\n",
        "    \n",
        "    model=Model(model.input,x)\n",
        "\n",
        "#To set the first 8 layers to non-trainable (weights will not be updated)\n",
        "\n",
        "    for layer in model.layers[:60]:\n",
        "       layer.trainable = False\n",
        "\n",
        "    for layer in model.layers[71:-6]:      \n",
        "      for attr in ['kernel_regularizer']:\n",
        "          setattr(layer, attr, tf.keras.regularizers.l1(0.2))\n",
        "\n",
        "    for layer in model.layers[71:-6]:      \n",
        "      for attr in ['bias_regularizer']:\n",
        "          setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        "# Learning rate is changed to 0.001\n",
        "    sgd = SGD(lr=2*1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osRRaWXr3hSk"
      },
      "source": [
        "#RESNET 256 pooling batchnorm, double droput\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "def ResNet(img_rows, img_cols, channel=1, num_classes=None):\n",
        "\n",
        "    model = ResNet50(weights='imagenet', include_top=True)\n",
        "\n",
        "    #model.layers.pop()\n",
        "\n",
        "    #model.outputs = [model.layers[-1].output]\n",
        "\n",
        "    model.layers[-1].outbound_nodes1 = []\n",
        "\n",
        "    x = tf.keras.layers.Dropout(.2)(model.layers[-35].output)\n",
        "\n",
        "    x=tf.keras.layers.Conv2D(256,1,kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "\n",
        "    x=tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x=Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "\n",
        "    x = tf.keras.layers.Dropout(.2)(x)\n",
        "\n",
        "    x=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x=Dense(num_classes, activation='softmax')(x)\n",
        "    \n",
        "    model=Model(model.input,x)\n",
        "\n",
        "# #To set the first 8 layers to non-trainable (weights will not be updated)\n",
        "\n",
        "#     for layer in model.layers[:60]:\n",
        "\n",
        "#        layer.trainable = False\n",
        "\n",
        "#     # adding regularization\n",
        "#     regularizer = tf.keras.regularizers.l2(0.01)\n",
        "\n",
        "#     for layer in model.layers[60:]:      \n",
        "#       for attr in ['kernel_regularizer']:\n",
        "#           if hasattr(layer, attr):\n",
        "#             setattr(layer, attr, regularizer)\n",
        "\n",
        "# # Learning rate is changed to 0.001\n",
        "#     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2s-RUZWM8TO",
        "outputId": "bb2067da-bed6-4c97-b939-6b4dfcecd3d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# RESSS\n",
        "\n",
        "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
        "channel = 3\n",
        "num_classes = 4 \n",
        "batch_size = 16\n",
        "nb_epoch = 50\n",
        "\n",
        "# Load our model\n",
        "model = ResNet(img_rows, img_cols, channel, num_classes)\n",
        "layer_names=[layer.name for layer in model.layers]\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 14, 14, 1024) 0           conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 14, 14, 256)  262400      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 14, 14, 256)  1024        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 14, 14, 256)  65792       batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 14, 14, 256)  0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 256)          0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 4)            1028        global_average_pooling2d_3[0][0] \n",
            "==================================================================================================\n",
            "Total params: 8,919,428\n",
            "Trainable params: 8,888,324\n",
            "Non-trainable params: 31,104\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXGNXolvgoIv",
        "outputId": "789f4b47-670a-4814-8d52-c5311e411eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "layer_names[81]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'conv4_block1_1_conv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JLXNvz7OXAi",
        "outputId": "142e603d-636f-425a-d50c-7fc32e5214d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nb_epoch =10\n",
        "batch_size = 32\n",
        "\n",
        "for layer in model.layers[:-1]:\n",
        "  layer.trainable = False\n",
        "\n",
        "for layer in model.layers[-1:]:\n",
        "  layer.trainable = True\n",
        "\n",
        " # Learning rate is changed to 0.001\n",
        "sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=1e-2,\n",
        "#     decay_steps=1000,\n",
        "#     decay_rate=0.98)\n",
        "\n",
        "# sgd = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_aug, Y_aug,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n",
        "\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "nb_epoch = 5\n",
        "batch_size = 32\n",
        "\n",
        "for layer in model.layers[81:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "for layer in model.layers[81:]:      \n",
        "  for attr in ['kernel_regularizer']:\n",
        "      setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        "for layer in model.layers[81:]:      \n",
        "  for attr in ['bias_regularizer']:\n",
        "      setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        " # Learning rate is changed to 0.001\n",
        "sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_aug, Y_aug,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n",
        "\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "nb_epoch =20\n",
        "batch_size = 32\n",
        "\n",
        "#To set the first 60 layers to non-trainable (weights will not be updated)\n",
        "\n",
        "for layer in model.layers[:60]:\n",
        "  layer.trainable = False\n",
        "\n",
        "for layer in model.layers[60:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "    # adding regularization\n",
        "regularizer = tf.keras.regularizers.l2(0.01)\n",
        "\n",
        "for layer in model.layers[60:]:      \n",
        "  for attr in ['kernel_regularizer']:\n",
        "      if hasattr(layer, attr):\n",
        "        setattr(layer, attr, regularizer)\n",
        "\n",
        "# Learning rate is changed to 0.001\n",
        "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "# Start Fine-tuning\n",
        "model.fit(X_aug, Y_aug,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "240/240 [==============================] - 23s 97ms/step - loss: 7.7692 - accuracy: 0.5448 - val_loss: 7.4361 - val_accuracy: 0.7769\n",
            "Epoch 2/10\n",
            "240/240 [==============================] - 23s 96ms/step - loss: 7.5162 - accuracy: 0.6864 - val_loss: 7.3369 - val_accuracy: 0.7631\n",
            "Epoch 3/10\n",
            "240/240 [==============================] - 23s 94ms/step - loss: 7.4293 - accuracy: 0.7181 - val_loss: 7.2683 - val_accuracy: 0.7824\n",
            "Epoch 4/10\n",
            "240/240 [==============================] - 22s 93ms/step - loss: 7.3802 - accuracy: 0.7371 - val_loss: 7.1753 - val_accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "240/240 [==============================] - 23s 94ms/step - loss: 7.3521 - accuracy: 0.7435 - val_loss: 7.2881 - val_accuracy: 0.7576\n",
            "Epoch 6/10\n",
            "240/240 [==============================] - 23s 95ms/step - loss: 7.3322 - accuracy: 0.7456 - val_loss: 7.1598 - val_accuracy: 0.8127\n",
            "Epoch 7/10\n",
            "240/240 [==============================] - 23s 94ms/step - loss: 7.3125 - accuracy: 0.7487 - val_loss: 7.2144 - val_accuracy: 0.7906\n",
            "Epoch 8/10\n",
            "240/240 [==============================] - 23s 94ms/step - loss: 7.2963 - accuracy: 0.7627 - val_loss: 7.1181 - val_accuracy: 0.8485\n",
            "Epoch 9/10\n",
            "240/240 [==============================] - 23s 94ms/step - loss: 7.2864 - accuracy: 0.7679 - val_loss: 7.1019 - val_accuracy: 0.8320\n",
            "Epoch 10/10\n",
            "240/240 [==============================] - 23s 94ms/step - loss: 7.2773 - accuracy: 0.7639 - val_loss: 7.1000 - val_accuracy: 0.8430\n",
            "Epoch 1/5\n",
            "240/240 [==============================] - 38s 160ms/step - loss: 4.7213 - accuracy: 0.8936 - val_loss: 2.8595 - val_accuracy: 0.9477\n",
            "Epoch 2/5\n",
            "240/240 [==============================] - 37s 155ms/step - loss: 1.7347 - accuracy: 0.9844 - val_loss: 1.1490 - val_accuracy: 0.9532\n",
            "Epoch 3/5\n",
            "240/240 [==============================] - 37s 155ms/step - loss: 0.7171 - accuracy: 0.9773 - val_loss: 0.5167 - val_accuracy: 0.9614\n",
            "Epoch 4/5\n",
            "240/240 [==============================] - 37s 155ms/step - loss: 0.3227 - accuracy: 0.9822 - val_loss: 0.2970 - val_accuracy: 0.9669\n",
            "Epoch 5/5\n",
            "240/240 [==============================] - 37s 156ms/step - loss: 0.1339 - accuracy: 0.9933 - val_loss: 0.5588 - val_accuracy: 0.8733\n",
            "Epoch 1/20\n",
            "240/240 [==============================] - 45s 185ms/step - loss: 0.0860 - categorical_accuracy: 0.9958 - val_loss: 0.1591 - val_categorical_accuracy: 0.9725\n",
            "Epoch 2/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0724 - categorical_accuracy: 0.9982 - val_loss: 0.1419 - val_categorical_accuracy: 0.9807\n",
            "Epoch 3/20\n",
            "240/240 [==============================] - 44s 181ms/step - loss: 0.0631 - categorical_accuracy: 0.9996 - val_loss: 0.1369 - val_categorical_accuracy: 0.9807\n",
            "Epoch 4/20\n",
            "240/240 [==============================] - 44s 181ms/step - loss: 0.0575 - categorical_accuracy: 0.9995 - val_loss: 0.1270 - val_categorical_accuracy: 0.9807\n",
            "Epoch 5/20\n",
            "240/240 [==============================] - 44s 181ms/step - loss: 0.0523 - categorical_accuracy: 1.0000 - val_loss: 0.1223 - val_categorical_accuracy: 0.9807\n",
            "Epoch 6/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0493 - categorical_accuracy: 0.9996 - val_loss: 0.1248 - val_categorical_accuracy: 0.9752\n",
            "Epoch 7/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0449 - categorical_accuracy: 0.9996 - val_loss: 0.1204 - val_categorical_accuracy: 0.9780\n",
            "Epoch 8/20\n",
            "240/240 [==============================] - 44s 181ms/step - loss: 0.0420 - categorical_accuracy: 0.9995 - val_loss: 0.1152 - val_categorical_accuracy: 0.9835\n",
            "Epoch 9/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0399 - categorical_accuracy: 0.9995 - val_loss: 0.1090 - val_categorical_accuracy: 0.9780\n",
            "Epoch 10/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0359 - categorical_accuracy: 0.9997 - val_loss: 0.1008 - val_categorical_accuracy: 0.9807\n",
            "Epoch 11/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0335 - categorical_accuracy: 1.0000 - val_loss: 0.1061 - val_categorical_accuracy: 0.9780\n",
            "Epoch 12/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0304 - categorical_accuracy: 1.0000 - val_loss: 0.0984 - val_categorical_accuracy: 0.9807\n",
            "Epoch 13/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0295 - categorical_accuracy: 0.9993 - val_loss: 0.1048 - val_categorical_accuracy: 0.9807\n",
            "Epoch 14/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0311 - categorical_accuracy: 0.9988 - val_loss: 0.0933 - val_categorical_accuracy: 0.9862\n",
            "Epoch 15/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0267 - categorical_accuracy: 0.9997 - val_loss: 0.0930 - val_categorical_accuracy: 0.9835\n",
            "Epoch 16/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0244 - categorical_accuracy: 0.9997 - val_loss: 0.0976 - val_categorical_accuracy: 0.9752\n",
            "Epoch 17/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0222 - categorical_accuracy: 1.0000 - val_loss: 0.0963 - val_categorical_accuracy: 0.9862\n",
            "Epoch 18/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0214 - categorical_accuracy: 0.9999 - val_loss: 0.0872 - val_categorical_accuracy: 0.9835\n",
            "Epoch 19/20\n",
            "240/240 [==============================] - 43s 181ms/step - loss: 0.0199 - categorical_accuracy: 0.9999 - val_loss: 0.0867 - val_categorical_accuracy: 0.9835\n",
            "Epoch 20/20\n",
            "240/240 [==============================] - 44s 181ms/step - loss: 0.0188 - categorical_accuracy: 1.0000 - val_loss: 0.0900 - val_categorical_accuracy: 0.9835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4e57356550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u0YzwmvhOWK"
      },
      "source": [
        "import h5py\n",
        "model.save(\"/content/gdrive/My Drive/models/resnet_9835.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cISTUO2soWkw"
      },
      "source": [
        "test_path=\"/content/gdrive/My Drive/plant_data/images/\"\n",
        "test=pd.read_csv(\"/content/gdrive/My Drive/plant_data/test.csv\")\n",
        "\n",
        "X_test=[]\n",
        "for i in range(len(test)):\n",
        "\n",
        "    temp_img=image.load_img(test_path+test['image_id'][i]+\".jpg\",target_size=(224,224))\n",
        "\n",
        "    temp_img=image.img_to_array(temp_img)\n",
        "\n",
        "    X_test.append(temp_img)\n",
        "\n",
        "X_test=np.array(X_test)\n",
        "X_test=preprocess_input(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh9xvM3qoHFX",
        "outputId": "ed8b317c-0b9f-43b3-8e69-377385d55f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "\n",
        "sub = pd.read_csv(\"/content/gdrive/My Drive/plant_data/sample_submission.csv\")\n",
        "\n",
        "probs_efnns = model.predict(X_test, verbose=1)\n",
        "sub.loc[:, 'healthy':] = probs_efnns\n",
        "sub.to_csv('/content/gdrive/My Drive/submission1.csv', index=False)\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 5s 90ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>healthy</th>\n",
              "      <th>multiple_diseases</th>\n",
              "      <th>rust</th>\n",
              "      <th>scab</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Test_0</td>\n",
              "      <td>1.251103e-07</td>\n",
              "      <td>0.002524</td>\n",
              "      <td>9.974754e-01</td>\n",
              "      <td>4.800383e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Test_1</td>\n",
              "      <td>1.303451e-07</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>9.999354e-01</td>\n",
              "      <td>4.306812e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test_2</td>\n",
              "      <td>1.020963e-07</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>6.043170e-08</td>\n",
              "      <td>9.999923e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Test_3</td>\n",
              "      <td>9.997217e-01</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>1.939462e-04</td>\n",
              "      <td>7.995026e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Test_4</td>\n",
              "      <td>1.154335e-09</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>9.999975e-01</td>\n",
              "      <td>3.478283e-09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  image_id       healthy  multiple_diseases          rust          scab\n",
              "0   Test_0  1.251103e-07           0.002524  9.974754e-01  4.800383e-07\n",
              "1   Test_1  1.303451e-07           0.000064  9.999354e-01  4.306812e-07\n",
              "2   Test_2  1.020963e-07           0.000008  6.043170e-08  9.999923e-01\n",
              "3   Test_3  9.997217e-01           0.000004  1.939462e-04  7.995026e-05\n",
              "4   Test_4  1.154335e-09           0.000002  9.999975e-01  3.478283e-09"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJuI1shLb_i5"
      },
      "source": [
        "# Make predictions\n",
        "predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Cross-entropy loss score\n",
        "score = log_loss(Y_valid, predictions_valid)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JRmVnEiI89O",
        "outputId": "22fe6a11-f37b-48e0-bb03-7f46fb7364fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.layers[-35]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f1f115553c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COtyaTn9imAl"
      },
      "source": [
        "model.outputs = [model.layers[-1].output]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZRZOoWnir1Y"
      },
      "source": [
        "model.layers[-1].output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9G4BynfYcFW"
      },
      "source": [
        "from tensorflow.keras.applications import DenseNet121\n",
        "from keras.models import Model\n",
        "\n",
        "def DenseNet(img_rows, img_cols, channel=1, num_classes=None):   \n",
        "      \n",
        "    model =DenseNet121(weights='imagenet',include_top=True)\n",
        "\n",
        "    x = tf.keras.layers.Dropout(.2)(model.layers[-117].output)\n",
        "\n",
        "    x=tf.keras.layers.Conv2D(256,1,kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "\n",
        "    x=tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x=Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "\n",
        "    x = tf.keras.layers.Dropout(.2)(x)\n",
        "\n",
        "    x=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x=Dense(num_classes, activation='softmax')(x)\n",
        "    \n",
        "    model=Model(model.input,x)\n",
        "\n",
        "#To set the first 8 layers to non-trainable (weights will not be updated)\n",
        "\n",
        "    for layer in model.layers[:60]:\n",
        "\n",
        "       layer.trainable = False\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tySU7sn7agxr",
        "outputId": "4ab75c6a-2511-48f2-8f5a-2f5457f24037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
        "channel = 3\n",
        "num_classes = 4 \n",
        "\n",
        "# Load our model\n",
        "model = DenseNet(img_rows, img_cols, channel, num_classes)\n",
        "layer_names=[layer.name for layer in model.layers]\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 7, 7, 512)    0           pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 7, 7, 256)    131328      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 7, 7, 256)    1024        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 7, 7, 256)    65792       batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 7, 7, 256)    0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_2 (Glo (None, 256)          0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 4)            1028        global_average_pooling2d_2[0][0] \n",
            "==================================================================================================\n",
            "Total params: 5,046,340\n",
            "Trainable params: 4,560,772\n",
            "Non-trainable params: 485,568\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wEVaZ2ck-yW",
        "outputId": "8a18170e-fd3b-456a-d01b-aac184f5de69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "layer_names[120]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'conv3_block10_1_relu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkrx6EDngS5w",
        "outputId": "125ae570-c344-4c51-a0fa-7d55691066bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nb_epoch =10\n",
        "batch_size = 32\n",
        "\n",
        "for layer in model.layers[:-1]:\n",
        "  layer.trainable = False\n",
        "\n",
        "for layer in model.layers[-1:]:\n",
        "  layer.trainable = True\n",
        "\n",
        " # Learning rate is changed to 0.001\n",
        "sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=1e-2,\n",
        "#     decay_steps=1000,\n",
        "#     decay_rate=0.98)\n",
        "\n",
        "# sgd = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_aug, Y_aug,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n",
        "\n",
        "###############################################################\n",
        "\n",
        "nb_epoch = 5\n",
        "batch_size = 32\n",
        "\n",
        "for layer in model.layers[120:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "for layer in model.layers[120:]:      \n",
        "  for attr in ['kernel_regularizer']:\n",
        "      setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        "for layer in model.layers[120:]:      \n",
        "  for attr in ['bias_regularizer']:\n",
        "      setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        " # Learning rate is changed to 0.001\n",
        "sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_aug, Y_aug,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n",
        "\n",
        "###############################################################\n",
        "\n",
        "nb_epoch = 5\n",
        "batch_size = 32\n",
        "\n",
        "for layer in model.layers[120:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "for layer in model.layers[120:]:      \n",
        "  for attr in ['kernel_regularizer']:\n",
        "      setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        "for layer in model.layers[120:]:      \n",
        "  for attr in ['bias_regularizer']:\n",
        "      setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        " # Learning rate is changed to 0.001\n",
        "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_aug, Y_aug,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n",
        "\n",
        "##############################################################\n",
        "\n",
        "nb_epoch = 10\n",
        "batch_size = 32\n",
        "\n",
        "for layer in model.layers[60:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "for layer in model.layers[60:]:      \n",
        "  for attr in ['kernel_regularizer']:\n",
        "      setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        "for layer in model.layers[60:]:      \n",
        "  for attr in ['bias_regularizer']:\n",
        "      setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        " # Learning rate is changed to 0.001\n",
        "sgd = SGD(lr=1e-4, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_aug, Y_aug,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "240/240 [==============================] - 56s 232ms/step - loss: 0.1270 - accuracy: 0.9752 - val_loss: 0.5449 - val_accuracy: 0.8623\n",
            "Epoch 2/10\n",
            "240/240 [==============================] - 54s 225ms/step - loss: 0.0989 - accuracy: 0.9860 - val_loss: 0.2550 - val_accuracy: 0.9311\n",
            "Epoch 3/10\n",
            "240/240 [==============================] - 54s 226ms/step - loss: 0.0917 - accuracy: 0.9888 - val_loss: 0.1942 - val_accuracy: 0.9532\n",
            "Epoch 4/10\n",
            "240/240 [==============================] - 54s 225ms/step - loss: 0.0795 - accuracy: 0.9929 - val_loss: 0.2008 - val_accuracy: 0.9532\n",
            "Epoch 5/10\n",
            "240/240 [==============================] - 54s 225ms/step - loss: 0.0782 - accuracy: 0.9941 - val_loss: 0.1965 - val_accuracy: 0.9587\n",
            "Epoch 6/10\n",
            "240/240 [==============================] - 54s 225ms/step - loss: 0.0738 - accuracy: 0.9958 - val_loss: 0.1939 - val_accuracy: 0.9559\n",
            "Epoch 7/10\n",
            "240/240 [==============================] - 54s 225ms/step - loss: 0.0726 - accuracy: 0.9942 - val_loss: 0.1966 - val_accuracy: 0.9532\n",
            "Epoch 8/10\n",
            "240/240 [==============================] - 54s 225ms/step - loss: 0.0688 - accuracy: 0.9959 - val_loss: 0.2020 - val_accuracy: 0.9559\n",
            "Epoch 9/10\n",
            "240/240 [==============================] - 54s 225ms/step - loss: 0.0681 - accuracy: 0.9970 - val_loss: 0.1906 - val_accuracy: 0.9559\n",
            "Epoch 10/10\n",
            "240/240 [==============================] - 54s 226ms/step - loss: 0.0668 - accuracy: 0.9970 - val_loss: 0.1960 - val_accuracy: 0.9614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4e5da2fdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcKT4cvMeUEC"
      },
      "source": [
        "import h5py\n",
        "model.save(\"/content/gdrive/My Drive/models/densenet_9614.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4d4ySU-KGlE"
      },
      "source": [
        "#VGG19 \n",
        "from keras.models import Model\n",
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "def VGG(img_rows, img_cols, channel=1, num_classes=None):\n",
        "\n",
        "    model = VGG19(weights='imagenet', include_top=True)\n",
        "\n",
        "    x=tf.keras.layers.Dropout(.2)(model.layers[-4].output)\n",
        "\n",
        "    x=model.layers[-3](x)\n",
        "\n",
        "    x=tf.keras.layers.Dropout(.2)(x)\n",
        "\n",
        "    x=model.layers[-2](x)\n",
        "\n",
        "    x=Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    #x=Dense(num_classes, activation='softmax')(model.layers[-2].output)\n",
        "    \n",
        "    model=Model(model.input,x)\n",
        "\n",
        "# Now stack everything back\n",
        "# Note: If you are going to fine tune the model, do not forget to\n",
        "#       mark other layers as un-trainable\n",
        "\n",
        "#To set the first 8 layers to non-trainable (weights will not be updated)\n",
        "\n",
        "#     for layer in model.layers[:-1]:\n",
        "\n",
        "#        layer.trainable = False\n",
        "\n",
        "# # Learning rate is changed to 0.001\n",
        "#     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyvm7YGS4HQL",
        "outputId": "48372e0a-f62b-4052-ea2a-4a38fcf2d736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# VGG\n",
        "\n",
        "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
        "channel = 3\n",
        "num_classes = 4 \n",
        "\n",
        "# Load our model\n",
        "model = VGG(img_rows, img_cols, channel, num_classes)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 16388     \n",
            "=================================================================\n",
            "Total params: 139,586,628\n",
            "Trainable params: 139,586,628\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE8nEk2mbbdH",
        "outputId": "b13c0274-8443-43f1-b11a-f265c5263a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nb_epoch = 10\n",
        "batch_size = 32\n",
        "\n",
        "for layer in model.layers[:-1]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Learning rate is changed to 0.001\n",
        "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 16388     \n",
            "=================================================================\n",
            "Total params: 134,276,932\n",
            "Trainable params: 16,388\n",
            "Non-trainable params: 134,260,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYQw0JKZk7Ob",
        "outputId": "d07905a7-de8e-40b7-a7ce-506ff498401f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nb_epoch =50\n",
        "batch_size = 16\n",
        "\n",
        "for layer in model.layers[:-1]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# # Learning rate is changed to 0.001\n",
        "# sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-2,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.98)\n",
        "\n",
        "sgd = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n",
        "# ##############################################################\n",
        "\n",
        "# nb_epoch = 40\n",
        "# batch_size = 16\n",
        "\n",
        "# for layer in model.layers[-3:]:\n",
        "#   layer.trainable = True\n",
        "\n",
        "# for layer in model.layers[-3:]:      \n",
        "#   for attr in ['kernel_regularizer']:\n",
        "#       setattr(layer, attr, tf.keras.regularizers.l1(0.2))\n",
        "\n",
        "# for layer in model.layers[-3:]:      \n",
        "#   for attr in ['bias_regularizer']:\n",
        "#       setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=1e-3,\n",
        "#     decay_steps=10000,\n",
        "#     decay_rate=0.98)\n",
        "\n",
        "# sgd = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "# model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n",
        "##############################################################\n",
        "# nb_epoch = 20\n",
        "# batch_size = 32\n",
        "\n",
        "# for layer in model.layers[-5:]:\n",
        "#   layer.trainable = True\n",
        "\n",
        "# for layer in model.layers[-5:]:      \n",
        "#   for attr in ['kernel_regularizer']:\n",
        "#       setattr(layer, attr, tf.keras.regularizers.l1(0.2))\n",
        "\n",
        "# for layer in model.layers[-5:]:      \n",
        "#   for attr in ['bias_regularizer']:\n",
        "#       setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=1e-4,\n",
        "#     decay_steps=10000,\n",
        "#     decay_rate=0.9)\n",
        "\n",
        "# sgd = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "# model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n",
        "# ##############################################################\n",
        "\n",
        "# nb_epoch = 10\n",
        "# batch_size = 32\n",
        "\n",
        "# for layer in model.layers[-7:]:\n",
        "#   layer.trainable = True\n",
        "\n",
        "# for layer in model.layers[-7:]:      \n",
        "#   for attr in ['kernel_regularizer']:\n",
        "#       setattr(layer, attr, tf.keras.regularizers.l1(0.2))\n",
        "\n",
        "# for layer in model.layers[-7:]:      \n",
        "#   for attr in ['bias_regularizer']:\n",
        "#       setattr(layer, attr, tf.keras.regularizers.l1(0.1))\n",
        "\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=1e-5,\n",
        "#     decay_steps=10000,\n",
        "#     decay_rate=0.9)\n",
        "\n",
        "# model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 3.7271 - accuracy: 0.4929 - val_loss: 0.8146 - val_accuracy: 0.7493\n",
            "Epoch 2/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.9718 - accuracy: 0.6377 - val_loss: 1.3891 - val_accuracy: 0.7107\n",
            "Epoch 3/50\n",
            "120/120 [==============================] - 12s 102ms/step - loss: 1.6163 - accuracy: 0.6623 - val_loss: 0.8740 - val_accuracy: 0.7383\n",
            "Epoch 4/50\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.2147 - accuracy: 0.7125 - val_loss: 2.2175 - val_accuracy: 0.5041\n",
            "Epoch 5/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.1682 - accuracy: 0.7297 - val_loss: 2.3047 - val_accuracy: 0.4931\n",
            "Epoch 6/50\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.1470 - accuracy: 0.7182 - val_loss: 1.3809 - val_accuracy: 0.6860\n",
            "Epoch 7/50\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.1999 - accuracy: 0.7219 - val_loss: 1.8122 - val_accuracy: 0.6667\n",
            "Epoch 8/50\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0172 - accuracy: 0.7412 - val_loss: 1.0293 - val_accuracy: 0.7603\n",
            "Epoch 9/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 1.1157 - accuracy: 0.7392 - val_loss: 3.7646 - val_accuracy: 0.5014\n",
            "Epoch 10/50\n",
            "120/120 [==============================] - 12s 100ms/step - loss: 1.1240 - accuracy: 0.7439 - val_loss: 0.8418 - val_accuracy: 0.7851\n",
            "Epoch 11/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.9988 - accuracy: 0.7585 - val_loss: 1.1043 - val_accuracy: 0.7493\n",
            "Epoch 12/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.9607 - accuracy: 0.7564 - val_loss: 0.8643 - val_accuracy: 0.7548\n",
            "Epoch 13/50\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 0.8428 - accuracy: 0.7831 - val_loss: 1.4079 - val_accuracy: 0.6722\n",
            "Epoch 14/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 1.0346 - accuracy: 0.7407 - val_loss: 1.9301 - val_accuracy: 0.6226\n",
            "Epoch 15/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.8686 - accuracy: 0.7747 - val_loss: 0.7066 - val_accuracy: 0.8182\n",
            "Epoch 16/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.8172 - accuracy: 0.7852 - val_loss: 0.9459 - val_accuracy: 0.7658\n",
            "Epoch 17/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.7847 - accuracy: 0.7893 - val_loss: 2.1969 - val_accuracy: 0.6887\n",
            "Epoch 18/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.7357 - accuracy: 0.7972 - val_loss: 0.8220 - val_accuracy: 0.7631\n",
            "Epoch 19/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.7246 - accuracy: 0.8024 - val_loss: 0.8209 - val_accuracy: 0.7576\n",
            "Epoch 20/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.8188 - accuracy: 0.7862 - val_loss: 0.7598 - val_accuracy: 0.7796\n",
            "Epoch 21/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.7599 - accuracy: 0.8019 - val_loss: 1.4206 - val_accuracy: 0.7025\n",
            "Epoch 22/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.6770 - accuracy: 0.8040 - val_loss: 0.8024 - val_accuracy: 0.7769\n",
            "Epoch 23/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.7757 - accuracy: 0.7888 - val_loss: 2.7844 - val_accuracy: 0.4683\n",
            "Epoch 24/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.7173 - accuracy: 0.8019 - val_loss: 1.7243 - val_accuracy: 0.6694\n",
            "Epoch 25/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.5898 - accuracy: 0.8322 - val_loss: 1.0937 - val_accuracy: 0.7273\n",
            "Epoch 26/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.6754 - accuracy: 0.7993 - val_loss: 0.8698 - val_accuracy: 0.7521\n",
            "Epoch 27/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.5939 - accuracy: 0.8186 - val_loss: 1.5548 - val_accuracy: 0.6556\n",
            "Epoch 28/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.5434 - accuracy: 0.8259 - val_loss: 2.0643 - val_accuracy: 0.6832\n",
            "Epoch 29/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.5562 - accuracy: 0.8369 - val_loss: 1.5307 - val_accuracy: 0.6309\n",
            "Epoch 30/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.5763 - accuracy: 0.8312 - val_loss: 1.4401 - val_accuracy: 0.6309\n",
            "Epoch 31/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.5640 - accuracy: 0.8254 - val_loss: 0.7071 - val_accuracy: 0.8017\n",
            "Epoch 32/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.5261 - accuracy: 0.8312 - val_loss: 0.9875 - val_accuracy: 0.7300\n",
            "Epoch 33/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.5604 - accuracy: 0.8244 - val_loss: 1.0011 - val_accuracy: 0.7521\n",
            "Epoch 34/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.5619 - accuracy: 0.8327 - val_loss: 1.7294 - val_accuracy: 0.5813\n",
            "Epoch 35/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.5333 - accuracy: 0.8359 - val_loss: 3.9632 - val_accuracy: 0.3609\n",
            "Epoch 36/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.5321 - accuracy: 0.8322 - val_loss: 0.5864 - val_accuracy: 0.8209\n",
            "Epoch 37/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.4790 - accuracy: 0.8484 - val_loss: 0.9061 - val_accuracy: 0.7245\n",
            "Epoch 38/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.4815 - accuracy: 0.8369 - val_loss: 1.2805 - val_accuracy: 0.7080\n",
            "Epoch 39/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.4382 - accuracy: 0.8573 - val_loss: 0.8708 - val_accuracy: 0.7521\n",
            "Epoch 40/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.5522 - accuracy: 0.8280 - val_loss: 0.6284 - val_accuracy: 0.8017\n",
            "Epoch 41/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.4565 - accuracy: 0.8400 - val_loss: 0.8641 - val_accuracy: 0.7906\n",
            "Epoch 42/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.5030 - accuracy: 0.8390 - val_loss: 0.9629 - val_accuracy: 0.7493\n",
            "Epoch 43/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.4585 - accuracy: 0.8505 - val_loss: 0.8353 - val_accuracy: 0.7521\n",
            "Epoch 44/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.4823 - accuracy: 0.8463 - val_loss: 0.9517 - val_accuracy: 0.7548\n",
            "Epoch 45/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.4068 - accuracy: 0.8615 - val_loss: 1.4759 - val_accuracy: 0.6116\n",
            "Epoch 46/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.4706 - accuracy: 0.8447 - val_loss: 0.8766 - val_accuracy: 0.7493\n",
            "Epoch 47/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.4107 - accuracy: 0.8568 - val_loss: 0.9390 - val_accuracy: 0.7438\n",
            "Epoch 48/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.3756 - accuracy: 0.8772 - val_loss: 1.2695 - val_accuracy: 0.6722\n",
            "Epoch 49/50\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 0.3701 - accuracy: 0.8709 - val_loss: 0.6627 - val_accuracy: 0.7989\n",
            "Epoch 50/50\n",
            "120/120 [==============================] - 12s 98ms/step - loss: 0.3718 - accuracy: 0.8745 - val_loss: 0.6138 - val_accuracy: 0.8099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6cfeaa9588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuJA6fmuWBnJ"
      },
      "source": [
        "#LR finder & CLR\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP51paNMWY6k"
      },
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# def cyclical_lr(step_sz, min_lr=0.001, max_lr=1, mode='triangular', scale_func=None, scale_md='cycles', gamma=1.):\n",
        "#     if scale_func == None:\n",
        "#         if mode == 'triangular':\n",
        "#             scale_fn = lambda x: 1.\n",
        "#             scale_mode = 'cycles'\n",
        "#         elif mode == 'triangular2':\n",
        "#             scale_fn = lambda x: 1 / (2.**(x - 1))\n",
        "#             scale_mode = 'cycles'\n",
        "#         elif mode == 'exp_range':\n",
        "#             scale_fn = lambda x: gamma**(x)\n",
        "#             scale_mode = 'iterations'\n",
        "#         else:\n",
        "#             raise ValueError(f'The {mode} is not valid value!')\n",
        "#     else:\n",
        "#         scale_fn = scale_func\n",
        "#         scale_mode = scale_md\n",
        "#     lr_lambda = lambda iters: min_lr + (max_lr - min_lr) * rel_val(iters, step_sz, scale_mode)\n",
        "#     def rel_val(iteration, stepsize, mode):\n",
        "#         cycle = math.floor(1 + iteration / (2 * stepsize))\n",
        "#         x = abs(iteration / stepsize - 2 * cycle + 1)\n",
        "#         if mode == 'cycles':\n",
        "#             return max(0, (1 - x)) * scale_fn(cycle)\n",
        "#         elif mode == 'iterations':\n",
        "#             return max(0, (1 - x)) * scale_fn(iteration)\n",
        "#         else:\n",
        "#             raise ValueError(f'The {scale_mode} is not valid value!')\n",
        "#     return lr_lambda\n",
        "# optimizer = optim.SGD(model.parameters(), lr=1.)\n",
        "# clr = cyclical_lr(step_size, min_lr=0.001, max_lr=1, mode='triangular2')\n",
        "# scheduler = lr_scheduler.LambdaLR(optimizer, [clr])\n",
        "# scheduler.step()\n",
        "# optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-QPHLpg-JHA",
        "outputId": "759409c5-0403-415a-b9de-79c40dcb6268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Keras da mevcut değil githubtan çağırıyoruz\n",
        "!git clone https://github.com/WittmannF/LRFinder.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: CLR in /usr/local/lib/python3.6/dist-packages (1.0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izi2K2tHHG2x",
        "outputId": "68c5bc0a-b821-44a5-9a22-4a2c2a5fd60d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "!git clone https://github.com/bckenstler/CLR.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CLR'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects:  12% (1/8)\u001b[K\rremote: Counting objects:  25% (2/8)\u001b[K\rremote: Counting objects:  37% (3/8)\u001b[K\rremote: Counting objects:  50% (4/8)\u001b[K\rremote: Counting objects:  62% (5/8)\u001b[K\rremote: Counting objects:  75% (6/8)\u001b[K\rremote: Counting objects:  87% (7/8)\u001b[K\rremote: Counting objects: 100% (8/8)\u001b[K\rremote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects:  14% (1/7)\u001b[K\rremote: Compressing objects:  28% (2/7)\u001b[K\rremote: Compressing objects:  42% (3/7)\u001b[K\rremote: Compressing objects:  57% (4/7)\u001b[K\rremote: Compressing objects:  71% (5/7)\u001b[K\rremote: Compressing objects:  85% (6/7)\u001b[K\rremote: Compressing objects: 100% (7/7)\u001b[K\rremote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "Receiving objects:   0% (1/252)   \rReceiving objects:   1% (3/252)   \rReceiving objects:   2% (6/252)   \rReceiving objects:   3% (8/252)   \rReceiving objects:   4% (11/252)   \rReceiving objects:   5% (13/252)   \rReceiving objects:   6% (16/252)   \rReceiving objects:   7% (18/252)   \rReceiving objects:   8% (21/252)   \rReceiving objects:   9% (23/252)   \rReceiving objects:  10% (26/252)   \rReceiving objects:  11% (28/252)   \rReceiving objects:  12% (31/252)   \rReceiving objects:  13% (33/252)   \rReceiving objects:  14% (36/252)   \rReceiving objects:  15% (38/252)   \rReceiving objects:  16% (41/252)   \rReceiving objects:  17% (43/252)   \rReceiving objects:  18% (46/252)   \rReceiving objects:  19% (48/252)   \rReceiving objects:  20% (51/252)   \rReceiving objects:  21% (53/252)   \rReceiving objects:  22% (56/252)   \rReceiving objects:  23% (58/252)   \rReceiving objects:  24% (61/252)   \rReceiving objects:  25% (63/252)   \rReceiving objects:  26% (66/252)   \rReceiving objects:  27% (69/252)   \rReceiving objects:  28% (71/252)   \rReceiving objects:  29% (74/252)   \rReceiving objects:  30% (76/252)   \rReceiving objects:  31% (79/252)   \rReceiving objects:  32% (81/252)   \rReceiving objects:  33% (84/252)   \rReceiving objects:  34% (86/252)   \rReceiving objects:  35% (89/252)   \rReceiving objects:  36% (91/252)   \rReceiving objects:  37% (94/252)   \rReceiving objects:  38% (96/252)   \rReceiving objects:  39% (99/252)   \rReceiving objects:  40% (101/252)   \rReceiving objects:  41% (104/252)   \rReceiving objects:  42% (106/252)   \rReceiving objects:  43% (109/252)   \rReceiving objects:  44% (111/252)   \rReceiving objects:  45% (114/252)   \rReceiving objects:  46% (116/252)   \rReceiving objects:  47% (119/252)   \rReceiving objects:  48% (121/252)   \rReceiving objects:  49% (124/252)   \rReceiving objects:  50% (126/252)   \rReceiving objects:  51% (129/252)   \rReceiving objects:  52% (132/252)   \rReceiving objects:  53% (134/252)   \rReceiving objects:  54% (137/252)   \rReceiving objects:  55% (139/252)   \rReceiving objects:  56% (142/252)   \rReceiving objects:  57% (144/252)   \rReceiving objects:  58% (147/252)   \rReceiving objects:  59% (149/252)   \rReceiving objects:  60% (152/252)   \rReceiving objects:  61% (154/252)   \rReceiving objects:  62% (157/252)   \rReceiving objects:  63% (159/252)   \rReceiving objects:  64% (162/252)   \rReceiving objects:  65% (164/252)   \rReceiving objects:  66% (167/252)   \rReceiving objects:  67% (169/252)   \rReceiving objects:  68% (172/252)   \rReceiving objects:  69% (174/252)   \rReceiving objects:  70% (177/252)   \rReceiving objects:  71% (179/252)   \rReceiving objects:  72% (182/252)   \rReceiving objects:  73% (184/252)   \rReceiving objects:  74% (187/252)   \rReceiving objects:  75% (189/252)   \rReceiving objects:  76% (192/252)   \rReceiving objects:  77% (195/252)   \rReceiving objects:  78% (197/252)   \rReceiving objects:  79% (200/252)   \rReceiving objects:  80% (202/252)   \rReceiving objects:  81% (205/252)   \rReceiving objects:  82% (207/252)   \rReceiving objects:  83% (210/252)   \rReceiving objects:  84% (212/252)   \rReceiving objects:  85% (215/252)   \rReceiving objects:  86% (217/252)   \rReceiving objects:  87% (220/252)   \rReceiving objects:  88% (222/252)   \rReceiving objects:  89% (225/252)   \rReceiving objects:  90% (227/252)   \rReceiving objects:  91% (230/252)   \rReceiving objects:  92% (232/252)   \rReceiving objects:  93% (235/252)   \rReceiving objects:  94% (237/252)   \rReceiving objects:  95% (240/252)   \rReceiving objects:  96% (242/252)   \rReceiving objects:  97% (245/252)   \rReceiving objects:  98% (247/252)   \rReceiving objects:  99% (250/252)   \rremote: Total 252 (delta 1), reused 4 (delta 0), pack-reused 244\u001b[K\n",
            "Receiving objects: 100% (252/252)   \rReceiving objects: 100% (252/252), 2.06 MiB | 28.11 MiB/s, done.\n",
            "Resolving deltas:   0% (0/87)   \rResolving deltas:  29% (26/87)   \rResolving deltas:  34% (30/87)   \rResolving deltas:  37% (33/87)   \rResolving deltas:  41% (36/87)   \rResolving deltas:  43% (38/87)   \rResolving deltas:  44% (39/87)   \rResolving deltas:  47% (41/87)   \rResolving deltas:  65% (57/87)   \rResolving deltas:  66% (58/87)   \rResolving deltas:  71% (62/87)   \rResolving deltas:  80% (70/87)   \rResolving deltas:  87% (76/87)   \rResolving deltas:  89% (78/87)   \rResolving deltas:  90% (79/87)   \rResolving deltas:  91% (80/87)   \rResolving deltas:  94% (82/87)   \rResolving deltas:  97% (85/87)   \rResolving deltas:  98% (86/87)   \rResolving deltas: 100% (87/87)   \rResolving deltas: 100% (87/87), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkhpNaKu75-8",
        "outputId": "4a27cbc0-dc2f-4a26-f460-9c102f265dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## LR Finder DENEME\n",
        "\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from LRFinder.keras_callback import LRFinder\n",
        "\n",
        "\n",
        "\n",
        "# Configuration settings for LR finder\n",
        "start_lr = 1e-4\n",
        "end_lr = 1e0\n",
        "no_epochs = 50\n",
        "\n",
        "\n",
        "# Define LR finder callback\n",
        "lr_finder = LRFinder(min_lr=start_lr, max_lr=end_lr)\n",
        "\n",
        "# Perform LR finder\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, callbacks=[lr_finder], epochs=no_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'LRFinder' already exists and is not an empty directory.\n",
            "Epoch 1/50\n",
            " 2/91 [..............................] - ETA: 4s - loss: 0.1722 - accuracy: 0.9375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0192s vs `on_train_batch_end` time: 0.0850s). Check your callbacks.\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.1774 - accuracy: 0.9451\n",
            "Epoch 2/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1835 - accuracy: 0.9409\n",
            "Epoch 3/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1732 - accuracy: 0.9430\n",
            "Epoch 4/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1854 - accuracy: 0.9444\n",
            "Epoch 5/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1885 - accuracy: 0.9437\n",
            "Epoch 6/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1662 - accuracy: 0.9547\n",
            "Epoch 7/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.1757 - accuracy: 0.9471\n",
            "Epoch 8/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1790 - accuracy: 0.9382\n",
            "Epoch 9/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1835 - accuracy: 0.9437\n",
            "Epoch 10/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1849 - accuracy: 0.9389\n",
            "Epoch 11/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1726 - accuracy: 0.9471\n",
            "Epoch 12/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1726 - accuracy: 0.9485\n",
            "Epoch 13/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.1669 - accuracy: 0.9560\n",
            "Epoch 14/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1672 - accuracy: 0.9464\n",
            "Epoch 15/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1727 - accuracy: 0.9492\n",
            "Epoch 16/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.1993 - accuracy: 0.9437\n",
            "Epoch 17/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1668 - accuracy: 0.9457\n",
            "Epoch 18/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1843 - accuracy: 0.9437\n",
            "Epoch 19/50\n",
            "91/91 [==============================] - 11s 121ms/step - loss: 0.1798 - accuracy: 0.9430\n",
            "Epoch 20/50\n",
            "91/91 [==============================] - 11s 121ms/step - loss: 0.1916 - accuracy: 0.9396\n",
            "Epoch 21/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1661 - accuracy: 0.9492\n",
            "Epoch 22/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.1872 - accuracy: 0.9396\n",
            "Epoch 23/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.2206 - accuracy: 0.9258\n",
            "Epoch 24/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.2108 - accuracy: 0.9334\n",
            "Epoch 25/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.1916 - accuracy: 0.9361\n",
            "Epoch 26/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.2619 - accuracy: 0.9176\n",
            "Epoch 27/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.2214 - accuracy: 0.9251\n",
            "Epoch 28/50\n",
            "91/91 [==============================] - 11s 119ms/step - loss: 0.2063 - accuracy: 0.9361\n",
            "Epoch 29/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.2373 - accuracy: 0.9231\n",
            "Epoch 30/50\n",
            "91/91 [==============================] - 11s 121ms/step - loss: 0.3168 - accuracy: 0.9004\n",
            "Epoch 31/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.4130 - accuracy: 0.8716\n",
            "Epoch 32/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.4781 - accuracy: 0.8668\n",
            "Epoch 33/50\n",
            "91/91 [==============================] - 11s 121ms/step - loss: 0.6702 - accuracy: 0.8194\n",
            "Epoch 34/50\n",
            "91/91 [==============================] - 11s 120ms/step - loss: 0.8378 - accuracy: 0.8015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f431ae2f080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgUN25lyHP2Q",
        "outputId": "1c4d8957-914d-46c2-9b88-e833e7194396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set CLR options\n",
        "clr_step_size = int(4 * (len(X_train)/batch_size))\n",
        "base_lr = 1e-4\n",
        "max_lr = 1e-2\n",
        "mode='triangular'\n",
        "validation_split = 0.2\n",
        "verbosity = 1\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "# Define the callback\n",
        "clr = CyclicLR(base_lr=base_lr, max_lr=max_lr, step_size=clr_step_size, mode=mode)\n",
        "\n",
        "\n",
        "# Fit data to model\n",
        "history = model.fit(X_train, Y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=no_epochs,\n",
        "            verbose=verbosity,\n",
        "            validation_data=(X_valid, Y_valid),\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 2/91 [..............................] - ETA: 9s - loss: 0.0016 - accuracy: 1.0000    WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0164s vs `on_train_batch_end` time: 0.0944s). Check your callbacks.\n",
            "91/91 [==============================] - 8s 93ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.2331 - val_accuracy: 0.9397\n",
            "Epoch 2/50\n",
            "91/91 [==============================] - 8s 93ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.2376 - val_accuracy: 0.9452\n",
            "Epoch 3/50\n",
            "91/91 [==============================] - 9s 94ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.2528 - val_accuracy: 0.9342\n",
            "Epoch 4/50\n",
            "91/91 [==============================] - 9s 94ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.2373 - val_accuracy: 0.9425\n",
            "Epoch 5/50\n",
            "91/91 [==============================] - 9s 94ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.2367 - val_accuracy: 0.9397\n",
            "Epoch 6/50\n",
            "91/91 [==============================] - 9s 95ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.2471 - val_accuracy: 0.9370\n",
            "Epoch 7/50\n",
            "91/91 [==============================] - 9s 95ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.2558 - val_accuracy: 0.9425\n",
            "Epoch 8/50\n",
            "91/91 [==============================] - 9s 95ms/step - loss: 0.0035 - accuracy: 0.9986 - val_loss: 0.2458 - val_accuracy: 0.9342\n",
            "Epoch 9/50\n",
            "91/91 [==============================] - 9s 96ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.2367 - val_accuracy: 0.9397\n",
            "Epoch 10/50\n",
            "91/91 [==============================] - 9s 96ms/step - loss: 0.0041 - accuracy: 0.9979 - val_loss: 0.2327 - val_accuracy: 0.9507\n",
            "Epoch 11/50\n",
            "91/91 [==============================] - 9s 96ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.2477 - val_accuracy: 0.9397\n",
            "Epoch 12/50\n",
            "91/91 [==============================] - 9s 96ms/step - loss: 0.0035 - accuracy: 0.9986 - val_loss: 0.2442 - val_accuracy: 0.9425\n",
            "Epoch 13/50\n",
            "91/91 [==============================] - 9s 96ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2475 - val_accuracy: 0.9397\n",
            "Epoch 14/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.2434 - val_accuracy: 0.9425\n",
            "Epoch 15/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.2463 - val_accuracy: 0.9397\n",
            "Epoch 16/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0037 - accuracy: 0.9986 - val_loss: 0.2445 - val_accuracy: 0.9425\n",
            "Epoch 17/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.2437 - val_accuracy: 0.9425\n",
            "Epoch 18/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0055 - accuracy: 0.9979 - val_loss: 0.2500 - val_accuracy: 0.9479\n",
            "Epoch 19/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.2457 - val_accuracy: 0.9507\n",
            "Epoch 20/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.2493 - val_accuracy: 0.9479\n",
            "Epoch 21/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0058 - accuracy: 0.9979 - val_loss: 0.2971 - val_accuracy: 0.9233\n",
            "Epoch 22/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.2735 - val_accuracy: 0.9342\n",
            "Epoch 23/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0043 - accuracy: 0.9993 - val_loss: 0.2577 - val_accuracy: 0.9342\n",
            "Epoch 24/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.2488 - val_accuracy: 0.9397\n",
            "Epoch 25/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.2447 - val_accuracy: 0.9397\n",
            "Epoch 26/50\n",
            "91/91 [==============================] - 9s 97ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.2374 - val_accuracy: 0.9452\n",
            "Epoch 27/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 0.2140 - val_accuracy: 0.9507\n",
            "Epoch 28/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.2329 - val_accuracy: 0.9397\n",
            "Epoch 29/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0037 - accuracy: 0.9986 - val_loss: 0.2492 - val_accuracy: 0.9370\n",
            "Epoch 30/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.2554 - val_accuracy: 0.9397\n",
            "Epoch 31/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.2637 - val_accuracy: 0.9370\n",
            "Epoch 32/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.2489 - val_accuracy: 0.9397\n",
            "Epoch 33/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0078 - accuracy: 0.9966 - val_loss: 0.2337 - val_accuracy: 0.9342\n",
            "Epoch 34/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.2508 - val_accuracy: 0.9315\n",
            "Epoch 35/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.2360 - val_accuracy: 0.9425\n",
            "Epoch 36/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.2325 - val_accuracy: 0.9370\n",
            "Epoch 37/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2267 - val_accuracy: 0.9397\n",
            "Epoch 38/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.2357 - val_accuracy: 0.9397\n",
            "Epoch 39/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 0.2724 - val_accuracy: 0.9342\n",
            "Epoch 40/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.2553 - val_accuracy: 0.9370\n",
            "Epoch 41/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2463 - val_accuracy: 0.9452\n",
            "Epoch 42/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.2382 - val_accuracy: 0.9397\n",
            "Epoch 43/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0023 - accuracy: 0.9986 - val_loss: 0.2400 - val_accuracy: 0.9452\n",
            "Epoch 44/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0020 - accuracy: 0.9986 - val_loss: 0.2372 - val_accuracy: 0.9425\n",
            "Epoch 45/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.2373 - val_accuracy: 0.9507\n",
            "Epoch 46/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.2830 - val_accuracy: 0.9260\n",
            "Epoch 47/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0031 - accuracy: 0.9986 - val_loss: 0.2773 - val_accuracy: 0.9315\n",
            "Epoch 48/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.2709 - val_accuracy: 0.9315\n",
            "Epoch 49/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.2724 - val_accuracy: 0.9315\n",
            "Epoch 50/50\n",
            "91/91 [==============================] - 9s 98ms/step - loss: 0.0050 - accuracy: 0.9979 - val_loss: 0.2660 - val_accuracy: 0.9342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGDkPHD_8fNc",
        "outputId": "a72e2e26-abd1-4034-b398-aed8373df9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "#tfa.optimizers.CyclicalLearningRate tfa e bağlı farklı bir CLR modülü\n",
        "\n",
        "\n",
        "# from CLR.clr_callback import CyclicLR\n",
        "# # Set CLR options\n",
        "# clr_step_size = int(4 * (len(X_train)/batch_size))\n",
        "# base_lr = 1e-4\n",
        "# max_lr = 1e-2\n",
        "# mode='cycle'\n",
        "# validation_split = 0.2\n",
        "# verbosity = 1\n",
        "\n",
        "# import tensorflow_addons as tfa\n",
        "# # Define the callback\n",
        "# clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=base_lr, maximal_learning_rate=max_lr, step_size=clr_step_size, scale_fn=callable, scale_mode=mode,name='CyclicalLearningRate')\n",
        "\n",
        "\n",
        "# # Fit data to model\n",
        "# history = model.fit(X_train, Y_train,\n",
        "#             batch_size=batch_size,\n",
        "#             epochs=no_epochs,\n",
        "#             verbose=verbosity,\n",
        "#             validation_split=validation_split,\n",
        "#             callbacks=[clr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-192-ce2a961518b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             callbacks=[clr])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             steps=data_handler.inferred_steps)\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, callbacks, add_history, add_progbar, model, **params)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CyclicalLearningRate' object has no attribute 'set_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g4_m8ut7v-q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FsJw5972dnj",
        "outputId": "1344326c-ee70-4f4d-f2c2-f92ea7b5c799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "layer_names[50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'conv3_block1_out'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxHTLcNhhxVq",
        "outputId": "b3ff89e8-64c4-4850-c75f-9e22cd99329b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "Y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJhbDCI-vOOv"
      },
      "source": [
        "#class weights dictionary calculator\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "y_integers = np.argmax(Y_train, axis=1)\n",
        "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
        "d_class_weights = dict(enumerate(class_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DzSQESQvjXe",
        "outputId": "b1b96bd6-36dc-4a4c-b6f8-3721a8f14c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "d_class_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.875, 1: 4.986301369863014, 2: 0.7250996015936255, 3: 0.7827956989247312}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9icvKXwoTO",
        "outputId": "17382074-7b75-4e71-c6cf-1a6214bd5722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#check counts of classes\n",
        "Y_train.sum(axis = 0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([416.,  73., 502., 465.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b-2dPyNsIyx",
        "outputId": "aefafe87-7fc4-4846-9bcc-ee4d84f7ed6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "# fit with class weights\n",
        "# model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid),class_weight=d_class_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a48a9f8b9896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start Fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_class_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'd_class_weights' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za7r4wc6q2ne",
        "outputId": "34054778-a4df-4692-f622-c7863a4f4d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Make predictions\n",
        "predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Cross-entropy loss score\n",
        "score = log_loss(Y_valid, predictions_valid)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 2/23 [=>............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0092s vs `on_predict_batch_end` time: 0.0455s). Check your callbacks.\n",
            "23/23 [==============================] - 1s 41ms/step\n",
            "0.27291393896936844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlRgDJZxVPpS",
        "outputId": "e236e4e6-eca8-4a59-db4a-b5766c3cfbb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "predictions_valid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.8694209e-06, 7.1015191e-01, 2.8983286e-01, 5.3199046e-06],\n",
              "       [9.6408325e-01, 4.2851796e-04, 1.6763131e-03, 3.3811890e-02],\n",
              "       [9.9805295e-01, 4.4308264e-05, 1.8862719e-03, 1.6570311e-05],\n",
              "       ...,\n",
              "       [6.5322148e-08, 3.9686561e-02, 9.6031326e-01, 9.5441543e-08],\n",
              "       [4.7499069e-07, 7.3452204e-01, 2.0961618e-01, 5.5861339e-02],\n",
              "       [1.4647809e-03, 2.1168798e-04, 6.6219559e-06, 9.9831688e-01]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    }
  ]
}